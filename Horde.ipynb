{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJWJlgKUAPxj"
      },
      "outputs": [],
      "source": [
        "# The horde url\n",
        "horde_url = \"https://stablehorde.net\"\n",
        "# Give a cool name to your instance\n",
        "horde_name = \"My Awesome Instance\"\n",
        "# The api_key identifies a unique user in the horde\n",
        "# Visit https://stablehorde.net/register to create one before you can join\n",
        "horde_api_key = \"0000000000\"\n",
        "# Put other users whose prompts you want to prioritize.\n",
        "# The owner's username is always included so you don't need to add it here, unless you want it to have lower priority than another user\n",
        "horde_priority_usernames = []\n",
        "# The amount of power your system can handle\n",
        "# 8 means 512*512. Each increase increases the possible resoluion by 64 pixes\n",
        "# So if you put this to 2 (the minimum, your SD can only generate 64x64 pixels\n",
        "# If you put this to 32, it is equivalent to 1024x1024 pixels\n",
        "horde_max_power = 8\n",
        "# Set this to false, if you do not want your worker to receive requests for NSFW generations\n",
        "horde_nsfw = True\n",
        "# A list of words which you do not want to your worker to accept\n",
        "horde_blacklist = []\n",
        "# A list of words for which you always want to allow the NSFW censor filter, even when this worker is in NSFW mode\n",
        "horde_censorlist = []\n",
        "\n",
        "%env USE_MEMORY_EFFICIENT_ATTENTION 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMIz5uopoyF9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Install dependencies\n",
        "!nvidia-smi\n",
        "# enable the following line if you want to speed up everything seems broken rn\n",
        "# !pip install \"git+https://github.com/facebookresearch/xformers\"\n",
        "\n",
        "!pip install diffusers==0.3.0\n",
        "!pip install transformers scipy ftfy\n",
        "!pip install gradio datasets tqdm\n",
        "!pip3 install Cython\n",
        "!mkdir /root/.huggingface\n",
        "!echo -n \"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\" > /root/.huggingface/token\n",
        "!git clone https://github.com/sberbank-ai/Real-ESRGAN\n",
        "!pip install -r Real-ESRGAN/requirements.txt\n",
        "# download model weights\n",
        "# x2 \n",
        "!gdown https://drive.google.com/uc?id=1pG2S3sYvSaO0V0B8QPOl1RapPHpUGOaV -O Real-ESRGAN/weights/RealESRGAN_x2.pth\n",
        "!gdown https://drive.google.com/uc?id=1SGHdZAln4en65_NQeQY9UjchtkEF9f5F -O Real-ESRGAN/weights/RealESRGAN_x4.pth\n",
        "!gdown https://drive.google.com/uc?id=1mT9ewx86PSrc43b-ax47l1E2UzR7Ln4j -O Real-ESRGAN/weights/RealESRGAN_x8.pth\n",
        "\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "import sys\n",
        "import re\n",
        "\n",
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "from torch import autocast\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    DDIMScheduler,\n",
        "    DiffusionPipeline,\n",
        "    PNDMScheduler,\n",
        "    UNet2DConditionModel,\n",
        ")\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "%cd Real-ESRGAN/\n",
        "from RealESRGAN import RealESRGAN\n",
        "%cd ..\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmi_ato09PYZ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Get an available GPU plus the system version\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.physical_device_desc.split(\",\")[1].split(\": \")[1] for x in local_device_protos if x.device_type == 'GPU']\n",
        "gpuname = get_available_gpus()[0]\n",
        "\n",
        "sysversion = sys.version[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpPcxhbYaxXC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Initialize img2img pipeline\n",
        "class StableDiffusionImg2ImgPipeline(DiffusionPipeline):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModel,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        scheduler: Union[DDIMScheduler, PNDMScheduler],\n",
        "        feature_extractor: CLIPFeatureExtractor,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            feature_extractor=feature_extractor,\n",
        "        )\n",
        "    def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n",
        "        r\"\"\"\n",
        "        Enable sliced attention computation.\n",
        "        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n",
        "        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n",
        "        Args:\n",
        "            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n",
        "                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n",
        "                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n",
        "                `attention_head_dim` must be a multiple of `slice_size`.\n",
        "        \"\"\"\n",
        "        if slice_size == \"auto\":\n",
        "            # half the attention head size is usually a good trade-off between\n",
        "            # speed and memory\n",
        "            slice_size = self.unet.config.attention_head_dim // 2\n",
        "        self.unet.set_attention_slice(slice_size)\n",
        "\n",
        "    def disable_attention_slicing(self):\n",
        "        r\"\"\"\n",
        "        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n",
        "        back to computing attention in one step.\n",
        "        \"\"\"\n",
        "        # set slice_size = `None` to disable `attention slicing`\n",
        "        self.enable_attention_slicing(None)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]],\n",
        "        init_image: Optional[torch.FloatTensor] = None,\n",
        "        strength: float = 0.8,\n",
        "        num_inference_steps: Optional[int] = 50,\n",
        "        guidance_scale: Optional[float] = 7.5,\n",
        "        eta: Optional[float] = 0.0,\n",
        "        generator: List[str] = [],\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        height: Optional[int] = 512,\n",
        "        width: Optional[int] = 512,\n",
        "    ):\n",
        "\n",
        "        if isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "        if strength < 0 or strength > 1:\n",
        "          raise ValueError(f'The value of strength should in [0.0, 1.0] but is {strength}')\n",
        "\n",
        "        if height % 8 != 0 or width % 8 != 0:\n",
        "            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "        # get prompt text embeddings\n",
        "        text_input = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "        # set timesteps\n",
        "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "        extra_set_kwargs = {}\n",
        "        offset = 0\n",
        "        if accepts_offset:\n",
        "            offset = 1\n",
        "            extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "        \n",
        "        if(init_image is not None):\n",
        "          self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "          # encode the init image into latents and scale the latents\n",
        "          init_latents = self.vae.encode(init_image.to(self.device)).latent_dist.sample()\n",
        "          init_latents = 0.18215 * init_latents\n",
        "\n",
        "          # prepare init_latents noise to latents\n",
        "          init_latents = torch.cat([init_latents] * batch_size)\n",
        "          \n",
        "          # get the original timestep using init_timestep\n",
        "          init_timestep = int(num_inference_steps * strength) + offset\n",
        "          init_timestep = min(init_timestep, num_inference_steps)\n",
        "          timesteps = self.scheduler.timesteps[-init_timestep]\n",
        "          timesteps = torch.tensor([timesteps] * batch_size, dtype=torch.long, device=self.device)\n",
        "          \n",
        "          # add noise to latents using the timesteps\n",
        "          noise = torch.randn(init_latents.shape, generator=torch.Generator(\"cuda\").manual_seed(int(generator[0])), device=self.device)\n",
        "          init_latents = self.scheduler.add_noise(init_latents, noise, timesteps)\n",
        "          latents = init_latents\n",
        "        else:\n",
        "          init_timestep = 0\n",
        "          latents = []\n",
        "          for seedt in generator:\n",
        "            gen = torch.Generator(\"cuda\").manual_seed(int(seedt))\n",
        "            latents = latents + [torch.randn(\n",
        "                (1,self.unet.in_channels, height // 8, width // 8),\n",
        "                generator=gen,\n",
        "                device=self.device)]\n",
        "          latents = torch.cat(latents,dim=0)\n",
        "                # set timesteps\n",
        "          accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "          extra_set_kwargs = {}\n",
        "          if accepts_offset:\n",
        "              extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "          self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "      \n",
        "\n",
        "        \n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance:\n",
        "            max_length = text_input.input_ids.shape[-1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "            )\n",
        "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "\n",
        "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "        # and should be between [0, 1]\n",
        "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "        extra_step_kwargs = {}\n",
        "        if accepts_eta:\n",
        "            extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "        \n",
        "        t_start = max(num_inference_steps - init_timestep + offset, 0) if init_image is not None else 0\n",
        "        for i, t in tqdm(enumerate(self.scheduler.timesteps[t_start:])):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "\n",
        "            # predict the noise residual\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "            # perform guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "        # scale and decode the image latents with vae\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents).sample\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        return {\"sample\": image}\n",
        "device = \"cuda\"\n",
        "model_path = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "# Using DDIMScheduler as anexample,this also works with PNDMScheduler\n",
        "# uncomment this line if you want to use it.\n",
        "\n",
        "# scheduler = PNDMScheduler.from_config(model_path, subfolder=\"scheduler\", use_auth_token=True)\n",
        "\n",
        "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "img2imgpipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    model_path,\n",
        "    scheduler=scheduler,\n",
        "    revision=\"fp16\", \n",
        "    torch_dtype=torch.float16,\n",
        "    use_auth_token=True\n",
        ").to(device)\n",
        "pipe = img2imgpipe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAmbHbj_uYCe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Enable attention slicing\n",
        "pipe.enable_attention_slicing()\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7YM38tia0BQ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Load pipelines\n",
        "model2 = RealESRGAN(\"cuda\", scale = 2)\n",
        "model2.load_weights(f'Real-ESRGAN/weights/RealESRGAN_x2.pth')\n",
        "model8 = RealESRGAN(\"cuda\", scale = 8)\n",
        "model8.load_weights(f'Real-ESRGAN/weights/RealESRGAN_x8.pth')\n",
        "model4 = RealESRGAN(\"cuda\", scale = 4)\n",
        "model4.load_weights(f'Real-ESRGAN/weights/RealESRGAN_x4.pth')\n",
        "def upscaleFunc(image,mult):\n",
        "    if(mult==\"2\"):\n",
        "        return model2.predict(np.array(image))\n",
        "    if(mult == \"4\"):\n",
        "        return model4.predict(np.array(image))\n",
        "    if(mult == \"8\"):\n",
        "        return model8.predict(np.array(image))\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ABhC0q2FAPx5"
      },
      "outputs": [],
      "source": [
        "#@markdown # Initialize Stable Horde functions\n",
        "import urllib3, json \n",
        "http = urllib3.PoolManager()\n",
        "# horde stuff\n",
        "DEFAULT_HEADERS = {'content-type': 'text/plain'}\n",
        "REQUEST_FAILED = \"Request failed\"\n",
        "\n",
        "\n",
        "def makeHttpRequest(type, url, body = None, headers = DEFAULT_HEADERS):\n",
        "    try:\n",
        "        if type == \"GET\":\n",
        "            return http.request(\"GET\", url)\n",
        "        elif type == \"POST\":\n",
        "            return http.request(\"POST\", url, body=body, headers=headers)\n",
        "    except Exception as e:\n",
        "        return REQUEST_FAILED\n",
        "\n",
        "def getPop():\n",
        "    return makeHttpRequest(\"POST\",horde_url+\"/api/v2/generate/pop\",body=json.dumps({\"name\":horde_name,\"priority_usernames\":horde_priority_usernames,\"nsfw\":horde_nsfw,\"blacklist\":horde_blacklist,\"max_pixels\":1024*1024}),headers={'content-type': 'application/json',\"apikey\":horde_api_key})\n",
        "  \n",
        "def push(base64encodedimage,seed,pid):\n",
        "    return makeHttpRequest(\"POST\", f\"{horde_url}/api/v2/generate/submit\", body=json.dumps({\"id\":pid,\"generation\":base64encodedimage,\"seed\":seed}), headers={\"Content-Type\":\"application/json\", \"apikey\":horde_api_key})\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEErJFjlrSWS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Start Stable Horde Worker\n",
        "print_prompts = False #@param {type:\"boolean\"}\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "version = \"1.6\"\n",
        "batch = 4\n",
        "from torch import autocast\n",
        "from torchvision import transforms\n",
        "\n",
        "import os\n",
        "import base64\n",
        "import time\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import time\n",
        "from tqdm import tqdm, trange\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import base64\n",
        "\n",
        "def my_preprocess(image, mask):\n",
        "    #find a way to make it work with 512-1024 dimensions, problem arises with the mask tensor\n",
        "    image = image.resize((512, 512))\n",
        "\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.0 * image - 1.0\n",
        "\n",
        "def my_preprocess_mask(mask):\n",
        "    mask = mask.convert(\"L\")\n",
        "    mask = mask.resize((64,64), resample=PIL.Image.LANCZOS)\n",
        "    mask = np.array(mask).astype(np.float32) / 255.0\n",
        "    mask = np.tile(mask,(4,1,1))\n",
        "    mask = mask[None].transpose(0, 1, 2, 3) #what does this step do?\n",
        "    mask = torch.from_numpy(mask)\n",
        "    return mask\n",
        "\n",
        "def load_img_pil(base64text):\n",
        "    temp = BytesIO()\n",
        "    \n",
        "    temp.write(base64.b64decode(base64text))\n",
        "      \n",
        "    return Image.open(temp,\"r\").convert(\"RGB\")\n",
        "def load_img(base64text, h0, w0):\n",
        "    temp = BytesIO()\n",
        "    \n",
        "    temp.write(base64.b64decode(base64text))\n",
        "    \n",
        "    image = Image.open(temp,\"r\").convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "   \n",
        "    if(h0 is not None and w0 is not None):\n",
        "        h, w = h0, w0\n",
        "    \n",
        "    w, h = map(lambda x: x - x % 32, (w0, h0))  # resize to integer multiple of 32\n",
        "\n",
        "    print(f\"New image size ({w}, {h})\")\n",
        "    image = image.resize((w, h), resample = Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "def image_grid_rec(imgs):\n",
        "    newimgs = []\n",
        "    while len(imgs)>0:\n",
        "        newimgs = newimgs + [image_grid(imgs[:9],3,3)]\n",
        "        imgs = imgs[9:]\n",
        "    return image_grid(newimgs,3,3)\n",
        "\n",
        "while True:\n",
        "    print(\"atarting listen\")\n",
        "    fetchedNewPromptFromServer = False\n",
        "    while not fetchedNewPromptFromServer:\n",
        "        serverData = getPop()\n",
        "        if serverData == REQUEST_FAILED:\n",
        "            print(\"failed\")\n",
        "            time.sleep(5)\n",
        "            continue\n",
        "\n",
        "        serverResponse = json.loads(serverData.data.decode(\"utf-8\"))\n",
        "        if(not \"id\" in serverResponse or serverResponse[\"id\"] is None):\n",
        "           time.sleep(5)\n",
        "           continue\n",
        "        else:\n",
        "            pid = serverResponse[\"id\"]\n",
        "            serverResponse = serverResponse[\"payload\"]\n",
        "            if print_prompts:\n",
        "                print(f\"Using prompt:{serverResponse['prompt']}\")\n",
        "            fetchedNewPromptFromServer = True\n",
        "\n",
        "   \n",
        "    \n",
        "        \n",
        "    \n",
        "    width = serverResponse[\"width\"]\n",
        "    height = serverResponse[\"height\"]\n",
        "    iterations = 1\n",
        "    cfg = 7.5\n",
        "    if(\"cfg_scale\" in serverResponse):\n",
        "        cfg = float(serverResponse[\"cfg_scale\"])\n",
        "        print(cfg)\n",
        "    prompt = [serverResponse[\"prompt\"]] * iterations\n",
        "    steps = serverResponse[\"ddim_steps\"]\n",
        "    inputimg = None\n",
        "    if(\"input\" in serverResponse):\n",
        "      inputimg = load_img(serverResponse[\"input\"],width,height)\n",
        "    upscale = None\n",
        "    if(\"upscale\" in serverResponse):\n",
        "        upscale = serverResponse[\"upscale\"]\n",
        "        inputimg = load_img_pil(serverResponse[\"input\"])\n",
        "    mask = None\n",
        "    if(\"mask\" in serverResponse):\n",
        "      mask = my_preprocess_mask(load_img_pil(serverResponse[\"mask\"]))\n",
        "      inputimg = my_preprocess(load_img_pil(serverResponse[\"input\"]),mask)\n",
        "    images = []\n",
        "    batches = 9\n",
        "    wid = max(width,height)\n",
        "    if( wid > 128):\n",
        "        batches = 6\n",
        "    if( wid > 256 ):\n",
        "        batches = 3\n",
        "    if( wid > 512 ):\n",
        "        batches = 1\n",
        "    with autocast(\"cuda\"):\n",
        "        \n",
        "        if(inputimg is not None):\n",
        "            if(upscale is not None):\n",
        "                images = [upscaleFunc(inputimg,upscale)]\n",
        "            else:\n",
        "                images = img2imgpipe(prompt=prompt, init_image=inputimg, strength=float(serverResponse[\"strength\"]), guidance_scale=7.5, generator=serverResponse[\"seed\"])[\"sample\"]\n",
        "        else:   \n",
        "            tot = len(prompt)\n",
        "            try:\n",
        "                  seed = int(serverResponse[\"seed\"])\n",
        "            except: \n",
        "              seed = random.randint(100,1000000)\n",
        "            seedd = [seed]  * iterations\n",
        "            origiseed = seedd\n",
        "            for i in range(0,iterations):\n",
        "                \n",
        "                seedd[i] = seedd[i] + i\n",
        "            try:\n",
        "                while(len(prompt) > 0):\n",
        "                    print(f\"Batch size:{batches}\")\n",
        "                    prog = tot-len(prompt)\n",
        "                    #makeHttpRequest(\"POST\", f\"https://writerbot.selkiemyth.com/update/{pid}?name=\"+nodename, body=f\"Iter {prog}-{prog+batches} started\")\n",
        "\n",
        "                    images = images + pipe(prompt[:batches],num_inference_steps=min(max(steps,10),150), generator=seedd[:batches], height=height,guidance_scale=cfg, width=width)[\"sample\"] # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "                    prompt = prompt[batches:]\n",
        "                    seedd = seedd[batches:]\n",
        "            except:\n",
        "                pass\n",
        "                #makeHttpRequest(\"POST\", f\"https://writerbot.selkiemyth.com/update/{pid}?name=\"+nodename, body=f\"Job Failed(retry in 10 mins\")\n",
        "\n",
        "            \n",
        "    #image = images[0] if (iterations == \"1\") else image_grid(images, 2, 2) if (iterations == \"4\") else image_grid(images, 3, 3) if (iterations == \"9\") else image_grid_rec(images)\n",
        "    # Now to display an image you can do either save it such as:\n",
        "    o = 0\n",
        "    for image in images:\n",
        "        temp = BytesIO()\n",
        "        image.save(temp,\"webp\")\n",
        "        encoded = base64.b64encode(temp.getvalue()).decode('utf-8')\n",
        "        saveFailed = push(encoded,origiseed[o],pid)\n",
        "        o+=1\n",
        "        if saveFailed == REQUEST_FAILED:\n",
        "            print(\"dammit\")\n",
        "\n",
        "    \"Finish generating images\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}