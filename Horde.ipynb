{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T10:16:40.959480Z",
     "iopub.status.busy": "2022-10-21T10:16:40.958817Z",
     "iopub.status.idle": "2022-10-21T10:16:45.497114Z",
     "shell.execute_reply": "2022-10-21T10:16:45.496253Z",
     "shell.execute_reply.started": "2022-10-21T10:16:40.959394Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/sd-webui/nataili.git\n",
    "%cd nataili\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class BridgeData(object):\n",
    "    def __init__(self):\n",
    "        random.seed()\n",
    "        self.horde_url = \"https://stablehorde.net\"\n",
    "        # Give a cool name to your instance\n",
    "        self.worker_name = f\"PRCGENNODE{random.randint(0,30000)}\" #use something unique IMPORTANT\n",
    "        # The api_key identifies a unique user in the horde\n",
    "        self.api_key = \"0000000000\"\n",
    "        # Put other users whose prompts you want to prioritize.\n",
    "        # The owner's username is always included so you don't need to add it here, unless you want it to have lower priority than another user\n",
    "        self.priority_usernames = []\n",
    "        self.max_power = 8\n",
    "        self.nsfw = True\n",
    "        self.censor_nsfw = False\n",
    "        self.blacklist = []\n",
    "        self.censorlist = []\n",
    "        self.allow_img2img = True\n",
    "        self.allow_unsafe_ip = True\n",
    "        self.model_names = [\"stable_diffusion\"]\n",
    "        self.max_pixels = 64*64*8*self.max_power\n",
    "        self.interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-21T10:18:22.791987Z",
     "iopub.status.busy": "2022-10-21T10:18:22.791617Z",
     "iopub.status.idle": "2022-10-21T10:19:23.116254Z",
     "shell.execute_reply": "2022-10-21T10:19:23.114000Z",
     "shell.execute_reply.started": "2022-10-21T10:18:22.791957Z"
    },
    "id": "5PG2yhyx7OHL",
    "outputId": "2dd6b0ff-4dce-407a-c9ae-1c8b128a52d5"
   },
   "outputs": [],
   "source": [
    "import json, requests, time, os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "#@markdown # Model Download/Load\n",
    "token = \"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\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Or\n",
    "Path_to_trained_model = \"\" #@param {type:\"string\"}\n",
    "#@markdown Insert the full path of your trained model (eg: /content/gdrive/MyDrive/zarathustra.ckpt) and it will automatically be placed in the right place, otherwise, leave it EMPTY (make sure there are no spaces in the path)\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "stable_diffusion_v1_5 = True #@param {type:\"boolean\"}\n",
    "#@markdown * Generalist AI image generating model. The baseline for all finetuned models.\n",
    "stable_diffusion_v1_4 = False #@param {type:\"boolean\"}\n",
    "#@markdown * Old version. Generalist AI image generating model. The baseline for all finetuned models.\n",
    "waifu_diffusion = False #@param {type:\"boolean\"}\n",
    "#@markdown * Anime styled generations.\n",
    "furry_epoch = False #@param {type:\"boolean\"}\n",
    "#@markdown * Furry styled generations.\n",
    "yiffy = False #@param {type:\"boolean\"}\n",
    "#@markdown * Furry styled generations.\n",
    "Zack3D = False #@param {type:\"boolean\"}\n",
    "#@markdown * Kink/NSFW oriented furry styled generations.\n",
    "trinart = False #@param {type:\"boolean\"}\n",
    "#@markdown * SFW Manga styled generations.\n",
    "\n",
    "%cd /nataili/\n",
    "\n",
    "!mkdir -p models/custom/\n",
    "!mkdir -p models/ldm/stable-diffusion-v1/\n",
    "\n",
    "models = json.load(open('./db.json'))\n",
    "dependencies = json.load(open('./db_dep.json'))\n",
    "remote_models = \"https://raw.githubusercontent.com/db0/nataili-model-reference/main/db.json\"\n",
    "remote_dependencies = \"https://raw.githubusercontent.com/db0/nataili-model-reference/main/db_dep.json\"\n",
    "try:\n",
    "  r = requests.get(remote_models)\n",
    "  models = r.json()\n",
    "  r = requests.get(remote_dependencies)\n",
    "  dependencies = dependencies\n",
    "except:\n",
    "  models = models\n",
    "  dependencies = dependencies\n",
    "\n",
    "def download_model(name):\n",
    "  if not name in models:\n",
    "    print(\"Model not found!\")\n",
    "\n",
    "  model = models[name]\n",
    "  download = model[\"config\"][\"download\"][0]\n",
    "\n",
    "  if 'file_url' in download:\n",
    "    download_url = download['file_url']\n",
    "    link = download_url.split(\"/\")\n",
    "    file_name = link[len(link) - 1]\n",
    "    if 'hf_auth' in download:\n",
    "      download_url = download_url.format(username=\"USER\", password=token)\n",
    "\n",
    "  if token == \"\" and \"hf_auth\" in download:\n",
    "    print('Huggingface token not supplied!')\n",
    "    exit()\n",
    "\n",
    "  model_destination = model[\"config\"][\"files\"][0][\"path\"]\n",
    "\n",
    "  !wget -O $model_destination $download_url\n",
    "  print(\"Downloaded model:\", name)\n",
    "\n",
    "if stable_diffusion_v1_5:\n",
    "  download_model(\"stable_diffusion\")\n",
    "if stable_diffusion_v1_4:\n",
    "  download_model(\"stable_diffusion_1.4\")\n",
    "if waifu_diffusion:\n",
    "  download_model(\"waifu_diffusion\")\n",
    "if furry_epoch:\n",
    "  download_model(\"waifu_diffusion\")\n",
    "if yiffy:\n",
    "  download_model(\"Yiffy\")\n",
    "if Zack3D:\n",
    "  download_model(\"Zack3D\")\n",
    "if trinart:\n",
    "  download_model(\"trinart\")\n",
    "\n",
    "if Path_to_trained_model != '':\n",
    "    if os.path.exists(str(Path_to_trained_model)):\n",
    "        clear_output()\n",
    "        !cp $Path_to_trained_model $required_path\n",
    "        if os.path.exists(required_path):\n",
    "            print('Model placed in the right directory')\n",
    "        else:\n",
    "            print('Something went wrong')\n",
    "    else:\n",
    "        print('Wrong path, use the colab file explorer to copy the path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2022-10-21T10:19:48.191993Z",
     "iopub.status.busy": "2022-10-21T10:19:48.191265Z",
     "iopub.status.idle": "2022-10-21T10:23:37.752442Z",
     "shell.execute_reply": "2022-10-21T10:23:37.751493Z",
     "shell.execute_reply.started": "2022-10-21T10:19:48.191963Z"
    },
    "id": "o-84LK908Yal",
    "outputId": "5948beee-8a18-4a48-d133-38eaacf9e622"
   },
   "outputs": [],
   "source": [
    "!pip install -e .\n",
    "\n",
    "# See: https://github.com/CompVis/taming-transformers/issues/176\n",
    "# do not uncomment -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
    "!pip install invisible-watermark==0.1.5\n",
    "!pip install taming-transformers-rom1504==0.0.6  # required by ldm\n",
    "\n",
    "!pip install git+https://github.com/crowsonkb/k-diffusion.git\n",
    "\n",
    "# Dependencies required for Stable Diffusion UI\n",
    "!pip install pynvml==11.4.1\n",
    "!pip install omegaconf==2.2.3\n",
    "\n",
    "# Note: Jinja2 3.x major version required due to breaking changes found in markupsafe==2.1.1; 2.0.1 is incompatible with other upstream dependencies\n",
    "# see https://github.com/pallets/markupsafe/issues/304\n",
    "!pip install Jinja2==3.1.2  # Jinja2 is required by Gradio\n",
    "\n",
    "!pip install diffusers==0.4.1\n",
    "\n",
    "# Img2text\n",
    "!pip install fairscale==0.4.4\n",
    "!pip install timm==0.6.7\n",
    "!pip install tqdm==4.64.0\n",
    "\n",
    "# Other\n",
    "!pip install retry==0.9.2  # used by sd_utils\n",
    "!pip install python-slugify==6.1.2  # used by sd_utils\n",
    "!pip install piexif==1.1.3  # used by sd_utils\n",
    "\n",
    "!pip install accelerate==0.12.0\n",
    "!pip install albumentations==0.4.3\n",
    "!pip install einops==0.3.1\n",
    "!pip install facexlib>=0.2.3\n",
    "!pip install imageio-ffmpeg==0.4.2\n",
    "!pip install imageio==2.9.0\n",
    "!pip install kornia==0.6\n",
    "!pip install loguru\n",
    "!pip install opencv-python-headless==4.6.0.66\n",
    "!pip install open-clip-torch==2.0.2\n",
    "!pip install pandas==1.4.3\n",
    "!pip install pudb==2019.2\n",
    "!pip install pytorch-lightning==1.7.7\n",
    "!pip install realesrgan==0.3.0\n",
    "!pip install test-tube>=0.7.5\n",
    "!pip install timm==0.6.7\n",
    "!pip install torch-fidelity==0.3.0\n",
    "!pip install transformers==4.19.2 # do not change\n",
    "!pip install wget\n",
    "\n",
    "# Upscalers\n",
    "!pip install basicsr==1.4.2  # required by RealESRGAN\n",
    "!pip install gfpgan==1.3.8  # GFPGAN\n",
    "!pip install realesrgan==0.3.0  # RealESRGAN brings in GFPGAN as a requirement\n",
    "!pip install git+https://github.com/CompVis/latent-diffusion\n",
    "\n",
    "## for monocular depth estimation \n",
    "!pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "execution": {
     "iopub.execute_input": "2022-10-21T10:23:37.756067Z",
     "iopub.status.busy": "2022-10-21T10:23:37.755741Z"
    },
    "id": "Hg3BkzLR6MIe",
    "outputId": "8fc5f7bd-1df7-4627-9b57-47ab80f23f9b"
   },
   "outputs": [],
   "source": [
    "import requests, json, os, time, argparse, urllib3, time,base64,re\n",
    "\n",
    "from nataili.inference.compvis.img2img import img2img\n",
    "from nataili.model_manager import ModelManager\n",
    "from nataili.inference.compvis.txt2img import txt2img\n",
    "from nataili.util.cache import torch_gc\n",
    "from nataili.util import logger,set_logger_verbosity, quiesce_logger, test_logger\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageFilter, ImageOps, ImageChops, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "from base64 import binascii\n",
    "\n",
    "import random\n",
    "model = ''\n",
    "max_content_length = 1024\n",
    "max_length = 80\n",
    "current_softprompt = None\n",
    "softprompts = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@logger.catch(reraise=True)\n",
    "def bridge(interval, model_manager, bd):\n",
    "    horde_url = bd.horde_url # Will replace later\n",
    "    current_id = None\n",
    "    current_payload = None\n",
    "    loop_retry = 0\n",
    "    while True:\n",
    "        if loop_retry > 10 and current_id:\n",
    "            logger.error(f\"Exceeded retry count {loop_retry} for generation id {current_id}. Aborting generation!\")\n",
    "            current_id = None\n",
    "            current_payload = None\n",
    "            current_generation = None\n",
    "            loop_retry = 0\n",
    "        elif current_id:\n",
    "            logger.debug(f\"Retrying ({loop_retry}/10) for generation id {current_id}...\")\n",
    "        available_models = model_manager.get_loaded_models_names()\n",
    "        gen_dict = {\n",
    "            \"name\": bd.worker_name,\n",
    "            \"max_pixels\": bd.max_pixels,\n",
    "            \"priority_usernames\": bd.priority_usernames,\n",
    "            \"nsfw\": bd.nsfw,\n",
    "            \"blacklist\": bd.blacklist,\n",
    "            \"models\": available_models,\n",
    "            \"allow_img2img\": bd.allow_img2img,\n",
    "            \"allow_unsafe_ip\": bd.allow_unsafe_ip,\n",
    "            \"bridge_version\": 3,\n",
    "        }\n",
    "        # logger.debug(gen_dict)\n",
    "        headers = {\"apikey\": bd.api_key}\n",
    "        if current_id:\n",
    "            loop_retry += 1\n",
    "        else:\n",
    "            try:\n",
    "                pop_req = requests.post(horde_url + '/api/v2/generate/pop', json = gen_dict, headers = headers)\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                logger.warning(f\"Server {horde_url} unavailable during pop. Waiting 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            except TypeError:\n",
    "                logger.warning(f\"Server {horde_url} unavailable during pop. Waiting 10 seconds...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            try:\n",
    "                pop = pop_req.json()\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                logger.error(f\"Could not decode response from {horde_url} as json. Please inform its administrator!\")\n",
    "                time.sleep(interval)\n",
    "                continue\n",
    "            if pop == None:\n",
    "                logger.error(f\"Something has gone wrong with {horde_url}. Please inform its administrator!\")\n",
    "                time.sleep(interval)\n",
    "                continue\n",
    "            if not pop_req.ok:\n",
    "                message = pop['message']\n",
    "                logger.warning(f\"During gen pop, server {horde_url} responded with status code {pop_req.status_code}: {pop['message']}. Waiting for 10 seconds...\")\n",
    "                if 'errors' in pop:\n",
    "                    logger.warning(f\"Detailed Request Errors: {pop['errors']}\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            if not pop.get(\"id\"):\n",
    "                skipped_info = pop.get('skipped')\n",
    "                if skipped_info and len(skipped_info):\n",
    "                    skipped_info = f\" Skipped Info: {skipped_info}.\"\n",
    "                else:\n",
    "                    skipped_info = ''\n",
    "                logger.debug(f\"Server {horde_url} has no valid generations to do for us.{skipped_info}\")\n",
    "                time.sleep(interval)\n",
    "                continue\n",
    "            current_id = pop['id']\n",
    "            current_payload = pop['payload']\n",
    "        model = pop.get(\"model\", available_models[0])\n",
    "        # logger.info([current_id,current_payload])\n",
    "        use_nsfw_censor = current_payload.get(\"use_nsfw_censor\", False)\n",
    "        if bd.censor_nsfw and not bd.nsfw:\n",
    "            use_nsfw_censor = True\n",
    "        elif any(word in current_payload['prompt'] for word in bd.censorlist):\n",
    "            use_nsfw_censor = True\n",
    "        use_gfpgan = current_payload.get(\"use_gfpgan\", True)\n",
    "        use_real_esrgan = current_payload.get(\"use_real_esrgan\", False)\n",
    "        source_image = pop.get(\"source_image\")\n",
    "        # These params will always exist in the payload from the horde\n",
    "        gen_payload = {\n",
    "            \"prompt\": current_payload[\"prompt\"],\n",
    "            \"height\": current_payload[\"height\"],\n",
    "            \"width\": current_payload[\"width\"],\n",
    "            \"width\": current_payload[\"width\"],\n",
    "            \"seed\": current_payload[\"seed\"],\n",
    "            \"n_iter\": 1,\n",
    "            \"batch_size\": 1,\n",
    "            \"save_individual_images\": False,\n",
    "            \"save_grid\": False,\n",
    "        }\n",
    "        # These params might not always exist in the horde payload\n",
    "        if 'ddim_steps' in current_payload: gen_payload['ddim_steps'] = current_payload['ddim_steps']\n",
    "        if 'sampler_name' in current_payload: gen_payload['sampler_name'] = current_payload['sampler_name']\n",
    "        if 'cfg_scale' in current_payload: gen_payload['cfg_scale'] = current_payload['cfg_scale']\n",
    "        if 'ddim_eta' in current_payload: gen_payload['ddim_eta'] = current_payload['ddim_eta']\n",
    "        if 'denoising_strength' in current_payload and source_image: \n",
    "            gen_payload['denoising_strength'] = current_payload['denoising_strength']\n",
    "        # logger.debug(gen_payload)\n",
    "        req_type = \"txt2img\"\n",
    "        if source_image:\n",
    "            req_type = \"img2img\"\n",
    "        logger.debug(f\"{req_type} ({model}) request with id {current_id} picked up. Initiating work...\")\n",
    "        try:\n",
    "            if source_image:\n",
    "                base64_bytes = source_image.encode('utf-8')\n",
    "                img_bytes = base64.b64decode(base64_bytes)\n",
    "                gen_payload['init_img'] = Image.open(BytesIO(img_bytes))\n",
    "                generator = img2img(model_manager.loaded_models[model][\"model\"], model_manager.loaded_models[model][\"device\"], 'bridge_generations', load_concepts=True, concepts_dir='models/custom/sd-concepts-library')\n",
    "            else:\n",
    "                generator = txt2img(model_manager.loaded_models[model][\"model\"], model_manager.loaded_models[model][\"device\"], 'bridge_generations', load_concepts=True, concepts_dir='models/custom/sd-concepts-library')\n",
    "        except KeyError:\n",
    "            continue\n",
    "        # If the received image is unreadable, we continue\n",
    "        except UnidentifiedImageError:\n",
    "            logger.error(f\"Source image received for img2img is unreadable. Falling back to text2img!\")\n",
    "            if 'denoising_strength' in gen_payload:\n",
    "                del gen_payload['denoising_strength']\n",
    "            generator = txt2img(model_manager.loaded_models[model][\"model\"], model_manager.loaded_models[model][\"device\"], 'bridge_generations', load_concepts=True, concepts_dir='models/custom/sd-concepts-library')\n",
    "        except binascii.Error:\n",
    "            logger.error(f\"Source image received for img2img is cannot be base64 decoded (binascii.Error). Falling back to text2img!\")\n",
    "            if 'denoising_strength' in gen_payload:\n",
    "                del gen_payload['denoising_strength']\n",
    "            generator = txt2img(model_manager.loaded_models[model][\"model\"], model_manager.loaded_models[model][\"device\"], 'bridge_generations', load_concepts=True, concepts_dir='models/custom/sd-concepts-library')\n",
    "        generator.generate(**gen_payload)\n",
    "        torch_gc()\n",
    "      \n",
    "\n",
    "\n",
    "        # images, seed, info, stats = txt2img(**current_payload)\n",
    "        buffer = BytesIO()\n",
    "        # We send as WebP to avoid using all the horde bandwidth\n",
    "        image = generator.images[0][\"image\"]\n",
    "        seed = generator.images[0][\"seed\"]\n",
    "        image.save(buffer, format=\"WebP\", quality=90)\n",
    "        # logger.info(info)\n",
    "        submit_dict = {\n",
    "            \"id\": current_id,\n",
    "            \"generation\": base64.b64encode(buffer.getvalue()).decode(\"utf8\"),\n",
    "            \"api_key\": bd.api_key,\n",
    "            \"seed\": seed,\n",
    "            \"max_pixels\": bd.max_pixels,\n",
    "        }\n",
    "        current_generation = seed\n",
    "        while current_id and current_generation != None:\n",
    "            try:\n",
    "                submit_req = requests.post(horde_url + '/api/v2/generate/submit', json = submit_dict, headers = headers)\n",
    "                try:\n",
    "                    submit = submit_req.json()\n",
    "                except json.decoder.JSONDecodeError:\n",
    "                    logger.error(f\"Something has gone wrong with {horde_url} during submit. Please inform its administrator!  (Retry {loop_retry}/10)\")\n",
    "                    time.sleep(interval)\n",
    "                    continue\n",
    "                if submit_req.status_code == 404:\n",
    "                    logger.warning(f\"The generation we were working on got stale. Aborting!\")\n",
    "                elif not submit_req.ok:\n",
    "                    logger.warning(f\"During gen submit, server {horde_url} responded with status code {submit_req.status_code}: {submit['message']}. Waiting for 10 seconds...  (Retry {loop_retry}/10)\")\n",
    "                    if 'errors' in submit:\n",
    "                        logger.warning(f\"Detailed Request Errors: {submit['errors']}\")\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                else:\n",
    "                    logger.info(f'Submitted generation with id {current_id} and contributed for {submit_req.json()[\"reward\"]}')\n",
    "                current_id = None\n",
    "                current_payload = None\n",
    "                current_generation = None\n",
    "                loop_retry = 0\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                logger.warning(f\"Server {horde_url} unavailable during submit. Waiting 10 seconds...  (Retry {loop_retry}/10)\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "        time.sleep(interval)\n",
    "\n",
    "@logger.catch(reraise=True)\n",
    "def check_models(models):\n",
    "    logger.init(\"Models\", status=\"Checking\")\n",
    "    from os.path import exists\n",
    "    import sys\n",
    "    mm = ModelManager()\n",
    "    models_exist = True\n",
    "    not_found_models = []\n",
    "    for model in models:\n",
    "        if not mm.get_model(model):\n",
    "            logger.err(f\"Model name requested {model} in bridgeData is unknown to us. Please check your configuration. Aborting!\")\n",
    "            sys.exit(1)\n",
    "        if not mm.validate_model(model):\n",
    "            models_exist = False\n",
    "            not_found_models.append(model)\n",
    "    if not models_exist:\n",
    "        choice = input(f\"You do not appear to have downloaded the models needed yet.\\nYou need at least a main model to proceed. Would you like to download your prespecified models?\\n\\\n",
    "        y: Download {not_found_models} (default).\\n\\\n",
    "        n: Abort and exit\\n\\\n",
    "        all: Download all models (This can take a significant amount of time and bandwidth)?\\n\\\n",
    "        Please select an option: \")\n",
    "        if choice not in ['y', 'Y', '', 'yes', 'all', 'a']:\n",
    "            sys.exit(1)\n",
    "        needs_hf = False\n",
    "        for model in not_found_models:\n",
    "            dl = mm.get_model_download(model)\n",
    "            for m in dl:\n",
    "                if 'huggingface.co' in m['file_url']:\n",
    "                    needs_hf = True\n",
    "        if needs_hf or choice in ['all', 'a']:\n",
    "            try:\n",
    "                from creds import hf_username,hf_password\n",
    "            except:\n",
    "                hf_username = input(\"Please type your huggingface.co username: \")\n",
    "                hf_password = input(\"Please type your huggingface.co Access Token or password: \")\n",
    "            hf_auth = {\"username\": hf_username, \"password\": hf_password}\n",
    "            mm.set_authentication(hf_auth=hf_auth)\n",
    "        mm.init()\n",
    "        mm.taint_models(not_found_models)\n",
    "        if choice in ['all', 'a']:\n",
    "            mm.download_all()    \n",
    "        elif choice in ['y', 'Y', '', 'yes']:\n",
    "            for model in not_found_models:\n",
    "                logger.init(f\"Model: {model}\", status=\"Downloading\")\n",
    "                if not mm.download_model(model):\n",
    "                    logger.message(\"Something went wrong when downloading the model and it does not fit the expected checksum. Please check that your HuggingFace authentication is correct and that you've accepted the model license from the browser.\")\n",
    "                    sys.exit(0)\n",
    "    logger.init_ok(\"Models\", status=\"OK\")\n",
    "    if exists('./bridgeData.py'):\n",
    "        logger.init_ok(\"Bridge Config\", status=\"OK\")\n",
    "    elif input(\"You do not appear to have a bridgeData.py. Would you like to create it from the template now? (y/n)\") in ['y', 'Y', '', 'yes']:\n",
    "        with open('bridgeData_template.py','r') as firstfile, open('bridgeData.py','a') as secondfile:\n",
    "            for line in firstfile:\n",
    "                secondfile.write(line)\n",
    "        logger.message(\"bridgeData.py created. Bridge will exit. Please edit bridgeData.py with your setup and restart the bridge\")\n",
    "        sys.exit(0)\n",
    "    \n",
    "def load_bridge_data():\n",
    "    bridge_data = BridgeData()\n",
    "    if bridge_data.max_power < 2:\n",
    "        bridge_data.max_power = 2\n",
    "    bridge_data.max_pixels = 64*64*8*bridge_data.max_power\n",
    "    return(bridge_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    bd = load_bridge_data()\n",
    "    # test_logger()\n",
    "    check_models(bd.model_names)\n",
    "    model_manager = ModelManager()\n",
    "    model_manager.init()\n",
    "    for model in bd.model_names:\n",
    "        logger.init(f'{model}', status=\"Loading\")\n",
    "        success = model_manager.load_model(model)\n",
    "        if success:\n",
    "            logger.init_ok(f'{model}', status=\"Loaded\")\n",
    "        else:\n",
    "            logger.init_err(f'{model}', status=\"Error\")\n",
    "    logger.init(f\"API Key '{bd.api_key}'. Server Name '{bd.worker_name}'. Horde URL '{bd.horde_url}'. Max Pixels {bd.max_pixels}\", status=\"Joining Horde\")\n",
    "    try:\n",
    "        bridge(bd.interval, model_manager, bd)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(f\"Keyboard Interrupt Received. Ending Process\")\n",
    "    logger.init(f\"{bd.worker_name} Instance\", status=\"Stopped\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
